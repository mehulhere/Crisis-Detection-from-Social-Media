{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the most recent tweets posted from geolocation: 11.059821, 78.387451 within 1000km\n",
      "‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞¨‡±Ä‡∞ú‡±á‡∞™‡±Ä ‡∞∂‡∞æ‡∞∏‡∞®‡∞∏‡∞≠‡±ç‡∞Ø‡±Å‡∞≤‡∞§‡±ã, ‡∞é‡∞Ç‡∞™‡±Ä‡∞≤‡∞§‡±ã ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞Æ‡∞Ç‡∞ö‡∞ø ‡∞∏‡∞Æ‡∞æ‡∞µ‡±á‡∞∂‡∞Ç ‡∞ú‡∞∞‡∞ø‡∞ó‡∞ø‡∞Ç‡∞¶‡∞ø.\n",
      "\n",
      "‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞Ç‡∞≤‡±ã ‡∞Æ‡∞æ ‡∞™‡∞æ‡∞∞‡±ç‡∞ü‡±Ä ‡∞â‡∞®‡∞ø‡∞ï‡∞ø ‡∞µ‡±á‡∞ó‡∞Ç‡∞ó‡∞æ ‡∞µ‡∞ø‡∞∏‡±ç‡∞§‡∞∞‡∞ø‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞ø. ‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞™‡±ç‡∞∞‡∞ú‡∞≤‡±Å ‡∞á‡∞™‡±ç‡∞™‡∞ü‡∞ø‡∞ï‡±á ‡∞ï‡∞æ‡∞Ç‡∞ó‡±ç‡∞∞‡±Ü‡∞∏‡±ç‚Äå‡∞§‡±ã ‡∞µ‡∞ø‡∞∏‡∞ø‡∞ó‡∞ø‡∞™‡±ã‡∞Ø‡∞æ‡∞∞‡±Å ‡∞Ö‡∞Ç‡∞§‡±á‡∞ï‡∞æ‡∞ï ‡∞¨‡±Ä‡∞Ü‡∞∞‡±ç‚Äå‡∞é‡∞∏‡±ç ‡∞¶‡±Å‡∞∑‡±ç‡∞ü‡∞™‡∞æ‡∞≤‡∞® ‡∞µ‡∞≤‡±ç‡∞≤ ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø‡∞® ‡∞≠‡∞Ø‡∞Ç‡∞ï‡∞∞‡∞Æ‡±à‡∞® ‡∞ú‡±ç‡∞û‡∞æ‡∞™‡∞ï‡∞æ‡∞≤‡∞§‡±ã ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. ‡∞é‡∞Ç‡∞§‡±ã ‡∞Ü‡∞∂‡∞§‡±ã ‡∞¨‡±Ä‡∞ú‡±á‡∞™‡±Ä ‡∞µ‡±à‡∞™‡±Å ‡∞ö‡±Ç‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å.\n",
      "You are so much more than the media now\n",
      "Indian culture resonates globally! \n",
      "\n",
      "Wherever I go, I see immense enthusiasm towards our history and culture, which is extremely gladdening. Here is a glimpse‚Ä¶\n",
      " Happy Thanksgiving! \n",
      "37-year-old Lionel Messi is still being nominated for The Best FIFA Men's Player in 2024.\n",
      "\n",
      "We're not worthy \n",
      "The Indian Cricket Team were hosted by the Honourable Anthony Albanese MP, Prime Minister of Australia at the Parliament House, Canberra. \n",
      "\n",
      "#TeamIndia will take part in a two-day pink ball match against PM XI starting Saturday.\n",
      "BIG BREAKING NEWS  PM Modi held high level meeting on Bangladesh crisis \n",
      "\n",
      "Major Action likely !! PM Modi held talks with EAM S Jaishankar.\n",
      "\n",
      "Indian Govt will also address Parliament regarding atrocities against Bangladeshi Hindus.\n",
      "\n",
      "PM Modi is reportedly upset. Modi Govt\n",
      "ùêçùêé ùêãùêéùêéùêä \n",
      "\"Just as a strong wind sweeps a boat off its chartered course on the water, even one of the senses on which the mind focuses can lead the intellect astray.\"\n",
      "\n",
      "BG 2.67 \n",
      "Van Dijk: ‚ÄúWhen I felt bored in the game I passed the ball to Mbappe and then took it back.‚Äù\n",
      "Bro has us scoring in the first minute of every game.\n",
      "\n",
      "He‚Äôs insane\n",
      "Hello Chief! As we know, this update has come with some bugs, including 2 major ones. One of them was causing War attacks to disconnect which caused a huge inconvenience to you and your Clan. As a way to apologise, we'd like to share with you a small token of appreciation, which\n",
      "What do you have to do to get a 10 \n",
      "\n",
      "Have yourself an evening, Charles De Ketelaere.\n",
      " Who‚Äôs been your Man of the Match? \n",
      "Bro Gave STF move on Faf du plessis\n",
      "On top and unstoppable!  The VIDAAMUYARCHI teaser is trending at No. 1 on YouTube.  Effort, determination, and triumph collide! \n",
      "\n",
      " https://youtu.be/Wtq3RRORVx4?si=mDkCnrFyA7bPDeSk‚Ä¶\n",
      "\n",
      "#Vidaamuyarchi In Cinemas worldwide from PONGAL 2025!\n",
      "\n",
      "#AjithKumar #MagizhThirumeni \n",
      "@LycaProductions\n",
      " #Subaskaran\n",
      "Its high time #EishaSingh should be exposed on #WeekendKaVaar \n",
      "\n",
      "From linking Chahat to Arfeen & spitting venom against Karan she keeps getting worse every day. \n",
      "\n",
      "#BiggBoss18\n",
      "SHAME ON PAPPU!\n",
      "‡§Ü‡§≠‡§æ‡§∞‡•§ ‡§Ü‡§≠‡§æ‡§∞‡•§ ‡§Ü‡§≠‡§æ‡§∞\n",
      "Fuck Everton, I want Arsenal now!!!\n",
      "JADON SANCHO TO MYKHAILO MUDRYK TO MAKE IT 2-0 CHELSEA \n",
      "\n",
      "TOP BINS \n",
      " Mazraoui on Amorim calling him a key player: ‚ÄúIf I can help the team, I would play anywhere‚Äù.\n",
      "\n",
      "‚ÄúI do my best, no matter the position‚Äù.\n",
      "#JUSTIN | '‡§ú‡•à‡§∏‡•á ‡§ú‡•Å‡§Æ‡•á ‡§ï‡•Ä ‡§®‡§Æ‡§æ‡§ú ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§è‡§ï ‡§¶‡§ø‡§® ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§ú‡§æ‡§á‡§è. ‡§π‡§ø‡§Ç‡§¶‡•Ç ‡§¨‡§Ç‡§ü‡•á ‡§π‡•Å‡§è ‡§π‡•à‡§Ç, ‡§π‡§ø‡§Ç‡§¶‡•Å‡§ì‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§π‡•ã‡§®‡§æ ‡§ú‡§∞‡•Ç‡§∞‡•Ä ‡§π‡•à'- ‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡§æ ‡§Æ‡•Å‡§ï‡•á‡§∂ ‡§ñ‡§®‡•ç‡§®‡§æ \n",
      "\n",
      "#SanatanDharma #MukeshKhanna #TNNCard #HinduTemple #Trending\n",
      "[] ATEEZ take their first win for 'Ice On My Teeth' on today's KBS Music Bank\n",
      "\n",
      "#IceOnMyTeeth1stWin #ÏóêÏù¥Ìã∞Ï¶à \n",
      "#ATEEZ25thWin #ATEEZ\n",
      "[Í≥µÏßÄ] V 'The Warmest Winter' EVENT ÏïàÎÇ¥ (+ENG/JPN/CHN)\n",
      "The Hindu Right Wing can‚Äôt talk without resorting to offensive language, reflecting the toxicity of their ideology. \n",
      "This vulgar rhetoric has degraded public discourse to new lows, making it nearly impossible to engage in a civil discussion without compromising one's own dignity\n",
      "Difference‚Ä¶.\n",
      "‡§ú‡§¨ ‡§§‡§ï ‡§®‡§∞‡§ó‡§ø‡§∏ ‡§î‡§∞ ‡§∏‡•Å‡§®‡•Ä‡§≤ ‡§¶‡§§‡•ç‡§§ ‡§ï‡§æ ‡§¨‡•á‡§ü‡§æ ‡§¶‡§æ‡§ä‡§¶ ‡§î‡§∞ ‡§õ‡•ã‡§ü‡§æ ‡§∂‡§ï‡•Ä‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ò‡•Ç‡§Æ ‡§∞‡§π‡§æ ‡§•‡§æ ‡§§‡§¨ ‡§§‡§ï ‡§Ö‡§∞‡§´‡§æ ‡§¨‡•Ä‡§¨‡•Ä ‡§ï‡•ã ‡§â‡§∏‡§™‡§∞ ‡§ó‡§∞‡•ç‡§µ ‡§•‡§æ, ‡§Ü‡§ú ‡§è‡§ï ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§∏‡§Ç‡§§ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ò‡•Ç‡§Æ‡§®‡•á ‡§®‡§ø‡§ï‡§≤‡§æ ‡§§‡•ã ‡§Ö‡§∞‡§´‡§æ ‡§¨‡•Ä‡§¨‡•Ä ‡§ë‡§´‡•á‡§Ç‡§° ‡§π‡•ã ‡§ó‡§à ‡§π‡•à\n",
      "Real Madrid midfielder Eduardo Camavinga will be out for around three weeks, a source has told ESPN, after he picked up a muscular injury in his left leg in Wednesday's 2-0 Champions League defeat to Liverpool.\n",
      "Virat Kohli is the only Indian Men's player to score Hundred in Pink Ball Test match. \n",
      "\n",
      "- The Greatest in Modern Era.\n",
      "Cheteshwar Pujara said, \"KL Rahul shouldn't bat lower than 3 in the upcoming Tests\".\n",
      "‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§ú‡§¨ ‡§∏‡§Ç‡§∏‡§¶ ‡§ö‡§≤‡§æ‡§®‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§§‡•ã ‡§µ‡§ø‡§™‡§ï‡•ç‡§∑ ‡§ï‡•á 146 ‡§∏‡§æ‡§∏‡§Ç‡§¶‡•ã‡§Ç ‡§ï‡•ã ‡§∏‡§∏‡•ç‡§™‡•á‡§Ç‡§° ‡§ï‡§∞‡§ï‡•á ‡§∏‡§Ç‡§∏‡§¶ ‡§ö‡§≤‡§æ‡§Ø‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à, ‡§¨‡§ø‡§≤ ‡§™‡§æ‡§∏ ‡§ï‡§∞‡§æ‡§Ø‡•á ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç, ‡§á‡§∏ ‡§¨‡§æ‡§∞ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ú‡§æ‡§®‡§§‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§∏‡§Ç‡§∏‡§¶ ‡§ö‡§≤‡•á‡§ó‡•Ä ‡§§‡•ã ‡§∏‡§Ç‡§≠‡§≤ ‡§™‡§∞ ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§π‡•ã‡§ó‡•Ä, ‡§Ö‡§°‡§æ‡§®‡•Ä ‡§™‡§∞ ‡§∏‡§µ‡§æ‡§≤ ‡§π‡•ã‡§Ç‡§ó‡•á ‡§á‡§∏‡§≤‡§ø‡§Ø‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§∏‡§Ç‡§∏‡§¶ ‡§®‡§π‡•Ä‡§Ç ‡§ö‡§≤‡§®‡•á ‡§¶‡•á ‡§∞‡§π‡•Ä ‡§π‡•à‡•§\n",
      "23 years since release in Japan\n",
      "KKR spending Rs 23.75 crore on Venkatesh Iyer almost confirms his captaincy. Having his MP coach Chandrakant Pandit by his side will help. Iyer has a chance to make it big, very big. #IPL2025\n",
      "#VidaamuyarchiTeaser - https://youtu.be/Wtq3RRORVx4?si=mDkCnrFyA7bPDeSk‚Ä¶\n",
      "Once again with dearest AK sir \n",
      "Lesssssgoooooo \n",
      "#MagizhThirumeni \n",
      "@trishtrashers\n",
      " @akarjunofficial\n",
      " @ReginaCassandra\n",
      " @LycaProductions\n",
      "Looks like It will be a solo grand release for #VidaaMuyarachi direct Tamil and a pan India movie directed by Shankar #Gamechanger Telugu & Tamil dubbed for 2025 Pongal ..\n",
      "Real Madrid absolutely dominates Liverpool \n",
      "Who‚Äôs been your Man of the Match? \n",
      "Bhuvneshwar Kumar on Instagram:\n",
      "\n",
      "\"Thank you Orange Army for shaping me into who I am today. I will carry this love and support with me forever \".\n",
      "27th December it is  #Sikandar\n",
      "\n",
      "Let the CELEBRATION BEGINS !  \n",
      "@BeingSalmanKhan\n",
      "Daily life of Indian Leftits \n",
      "\n",
      "-Hindu Bad, islam good\n",
      "-Diwali harm animals, Eid breed animals \n",
      "-Free Pilestine, where is Bangladesh?\n",
      "-Free Umar Khalid, Dilbar Negi who?\n",
      "-Free Sarjeel imam, where is Ankit Sharma?\n",
      "-I stand with Zubair, what happened to Kanhaiya Lal & Umesh Kohle?\n",
      "Ready for some action?  \n",
      "@MokshNandamuri\n",
      " \n",
      "\n",
      "#SIMBAisCOMING\n",
      "‚ÄúEnding the year with a surprise on the cover of Vogue Thailand from the most successful Thai female artist of 2024 on the world stage, Lisa-Lalisa Manoban‚Äù\n",
      "BTS, Jungkook and Jimin are the only kpop artists to have a #1 song on Billboard HOT 100.\n",
      "Ghana legend Stephen Appiah reveals how Mohammed Gago faked an injury to help him secure his first professional contract with Udinese \n",
      "Manuel Neuer intercepted a pass that was going towards Kim Min-jae and sent a ball over to Kingsley Coman on the left wing \n",
      "\n",
      "His world \n",
      " Lamine Yamal returns to regular training with Barcelona group, same for Ferran Torres.\n",
      "\n",
      "Ronald Ara√∫jo did only part of the training with the group.\n",
      " Lookin' good in blaugrana, \n",
      "@Guaje7Villa\n",
      "‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§µ‡§ï‡•ç‡§´ ‡§¨‡•ã‡§∞‡•ç‡§° ‡§ï‡•ã 10 ‡§ï‡§∞‡•ã‡§°‡§º ‡§∞‡•Å‡§™‡§Ø‡•á ‡§ï‡•á ‡§§‡§§‡•ç‡§ï‡§æ‡§≤ ‡§Ü‡§µ‡§Ç‡§ü‡§® ‡§ï‡•Ä ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä‡•§\n",
      "#Maharashtra #WaqfBoard #Mahayuti\n",
      "\n",
      "#Shankar & #Thiru experimented with Infrared camera to give these beautiful visuals..\n",
      "\n",
      "GlobalStar \n",
      "@AlwaysRamCharan\n",
      " pony tail looks & \n",
      "@advani_kiara\n",
      " shined in the song\n",
      "@MusicThaman\n",
      " good rendition of melody \n",
      "\n",
      "#NaaNaaHyaraanaa\n",
      "#Sorgavaasal(A) (3.5/5) - A Very Unique, Realistic & Engaging Crime Action Movie Set In A Prison Backdrop. Excellent Making. Perfect Casting. RJ Balaji & Selva Delivers A Superb Performance. 2nd Half Is Impressive. Background Score Is Good. Lite ah Confusing. If You're An Action\n",
      " Arne Slot tells Sky Italy: ‚ÄúThe penalty missed by Mo Salah was not due to distraction due to contract situation‚Äù.\n",
      "\n",
      "‚ÄúThis is not the case. The penalty miss can happen, it doesn‚Äôt mean that Mo is distracted by contract talks‚Äù.\n",
      "MAN FOR CRISIS - GLENN PHILIPS \n",
      "\n",
      "- When Kiwis lost a few important wickets on Day 1, Phillips stood tall and scored unbeaten 41 runs in the first innings.\n",
      "\n",
      "Searching for the most recent tweets mentioning: Tamil Nadu\n",
      "Next time expect you in Tamil Nadu\n",
      "I don't know why we are imitating Tamil Nadu kind of madness.\n",
      "I want the kind of respect for my kannada as equally as any other language of my country, but there limitations to the effect of using a language most of times.\n",
      "It's better to be a multilingual.\n",
      "PMK President Dr Anbumani Ramadoss criticized Tamil Nadu CM MK Stalin and demanded answers on alleged corruption links and meetings with Gautam Adani.\n",
      "\n",
      "#AdaniStalinSecretMeet\n",
      "Cyclone Fengal Alert! Tamil Nadu & Puducherry brace for heavy rain & strong winds. Schools shut down, and coastal areas are on high alert. Stay safe indoors & follow updates! #CycloneFengal #Cyclone #Cyclonicstorm #Puducherry #TamilNadu #BreakingNews\n",
      "HARDIK PANDYA IN SYED MUSHTAQ ALI 2024:\n",
      "\n",
      "- 74*(35) vs Gujarat.\n",
      "- 41*(21) vs Uttrakhand.\n",
      "- 69(30) vs Tamil Nadu.\n",
      "- 47(23) vs Tripura.\n",
      "\n",
      "Unreal consistency with bat by Hardik Pandya \n",
      "29.11.24 Kanchipuram District Led Video Van,\"Drugs Free Tamil Nadu\"Awareness Video Screening at, Kanchipuram Rayal http://Matric.Hr.Sec.School.\n",
      "\n",
      "#CMO |\n",
      "#Dy.CM |\n",
      "#Kanchipuram |\n",
      "#‡Æï‡Ææ‡Æû‡Øç‡Æö‡Æø‡Æ™‡ØÅ‡Æ∞‡ÆÆ‡Øç |\n",
      "#Drugs free Tamilnadu |\n",
      "#Kancheepuram |\n",
      "https://makkaladhikarammedia.com/will-the-tamil-nadu-government-protect-natural-mineral-resources-from-robbers-or-will-it-destroy-katharum-kamaya-goundanpatti-town-panchayat-farmers-public/‚Ä¶ Will the Tamil Nadu government protect natural mineral resources from robbers? Or will it destroy? - Katharum Kamaya Goundanpatti Town Panchayat Farmers, Public.\n",
      "Back to back defeats for Tamil Nadu in the Syed Mushtaq Ali Trophy. Qualification hopes all but over, need to win the next 3 matches to stand any chance.\n",
      "Solar power prices drop yearly, but Tamil Nadu government chooses costlier deals with Adani. Dr. Anbumani Ramadoss demands transparency. \n",
      "\n",
      "#AdaniStalinSecretMeet\n",
      "Why is Tamil Nadu importing expensive private power at ‚Çπ5-‚Çπ12/unit when thermal plants can generate at ‚Çπ3/unit? PMK raises valid concerns. \n",
      "\n",
      "#AdaniStalinSecretMeet\n",
      "But isn't that true, except for Tamil nadu and goa\n",
      "TIDCO‚Äôs joint venture, TREAT (Tiruchirappalli Engineering and Technology Cluster), is empowering Tamil Nadu‚Äôs MSMEs with advanced engineering and fabrication facilities. \n",
      "\n",
      "#BuildTN #VentureCatalyst #TIDCO #TREAT #Innovation #MSME\n",
      "Deep depression intensifies into cyclonic storm 'Fengal', set to hit North Tamil Nadu, Puducherry coast on November 30\n",
      "During its landfall, the cyclonic storm is likely to bring wind speeds of 75‚Äì80 kmph, with gusts reaching up to 90 kmph.\n",
      "You call the capital of Tamil Nadu as Chennai or still Madras? \n",
      "Tamil Nadu paarra. Aprm anga polam\n",
      "What about Tamil Nadu Mr. Ramesh? Why Congress protection traitor DMK?\n",
      " Welcome to Nature's Paradise ‚Äì Hummingbird Community Villas! \n",
      "\n",
      " Location: Limestone Hummingbird, 152, Near Sulakkal Mariamman Temple, Mettupalayam, Kovilpalayam, Tamil Nadu 642110\n",
      " Contact us: 063848 23232\n",
      "Come, be a part of our nature-filled community! #hummingbird\n",
      "I want to put it out into the ether that not knowing and speaking Hindi is seeming like it was always a very strong net negative for Tamil Nadu and tamils\n",
      "‡∞∏‡±à‡∞ï‡±ç‡∞≤‡±ã‡∞®‡±ç ‡∞´‡±Ü‡∞Ç‡∞ó‡∞≤‡±ç.. ‡∞§‡∞Æ‡∞ø‡∞≥‡∞®‡∞æ‡∞°‡±Å‡∞≤‡±ã ‡∞≠‡∞æ‡∞∞‡±Ä ‡∞µ‡∞∞‡±ç‡∞∑‡∞æ‡∞≤‡±Å 7 ‡∞ú‡∞ø‡∞≤‡±ç‡∞≤‡∞æ‡∞≤‡∞ï‡±Å ‡∞∞‡±Ü‡∞°‡±ç ‡∞Ö‡∞≤‡∞∞‡±ç‡∞ü‡±ç\n",
      "Heavy Rains Lashes Tamil Nadu Due To Cyclone Fengal\n",
      "#fengalcyclone #tamilnadurain #tamilnadu #sakshiNews\n",
      "Not this shit again. FEP benefited Maharashtra and Gujarat, not Tamil Nadu. Much TN's growth happened after 1992, when FEP ended.\n",
      "Not this shit again. FEP benefited Maharashtra and Gujarat, not Tamil Nadu. Much TN's growth happened after 1992, when FEP ended.\n",
      "40 years is enough to denationalize any region. TamilNadu, which does not have any quality coal & energy source during 1952-1992, benefited the most from FEP & decline of the industrial complexes of the North Indian states.\n",
      "A Heartfelt Moment!\n",
      "\n",
      "Miss Rachna S.G. from Tamil Nadu, the proud winner of the National Science Seminar 2024, held at \n",
      "@NSCMumbai\n",
      ", \n",
      "@ncsmgoi\n",
      ", \n",
      "@MinOfCultureGoI\n",
      " expresses her heartfelt gratitude to her teachers, parents, & supporters.\n",
      "\n",
      "#NSS #ProudMoment #AI\n",
      "‡∞∏‡±à‡∞ï‡±ç‡∞≤‡±ã‡∞®‡±ç ‡∞´‡±Ü‡∞Ç‡∞ó‡∞≤‡±ç.. ‡∞§‡∞Æ‡∞ø‡∞≥‡∞®‡∞æ‡∞°‡±Å‡∞≤‡±ã ‡∞≠‡∞æ‡∞∞‡±Ä ‡∞µ‡∞∞‡±ç‡∞∑‡∞æ‡∞≤‡±Å 7 ‡∞ú‡∞ø‡∞≤‡±ç‡∞≤‡∞æ‡∞≤‡∞ï‡±Å ‡∞∞‡±Ü‡∞°‡±ç ‡∞Ö‡∞≤‡∞∞‡±ç‡∞ü‡±ç\n",
      "Heavy Rains Lashes Tamil Nadu Due To Cyclone Fengal\n",
      "#fengalcyclone #tamilnadurain #tamilnadu #sakshiNews\n",
      "‡Æ®‡Æü‡Æø‡Æï‡Øà ‡Æö‡ÆÆ‡Æ®‡Øç‡Æ§‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æ§‡Æ®‡Øç‡Æ§‡Øà ‡Æú‡Øã‡Æö‡Æ™‡Øç ‡Æ™‡Æø‡Æ∞‡Æ™‡ØÅ ‡Æï‡Ææ‡Æ≤‡ÆÆ‡Ææ‡Æ©‡Ææ‡Æ∞‡Øç. ‡Æ§‡Æ©‡Æ§‡ØÅ ‡Æ§‡Æ®‡Øç‡Æ§‡Øà ‡Æá‡Æ±‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ ‡Æï‡ØÅ‡Æ±‡Æø‡Æ§‡Øç‡Æ§‡ØÅ 'Until we meet again Dad' ‡Æé‡Æ© ‡Æö‡ÆÆ‡Æ®‡Øç‡Æ§‡Ææ ‡Æâ‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ÆÆ‡Ææ‡Æï ‡Æ™‡Æ§‡Æø‡Æµ‡ØÅ! #SamanthaRuthPrabhu #Samantha\n",
      "Iam Bjp member in Tamil Nadu my vote only bjp my family vote only bjp my India my BJP my India Priminister Hon'ble modi powerful prime minister india summit Grievance petition No response only Answer and Rejected sir please help me sir Your GOD SIR Anyone help sir\n",
      "Current AQI of all CAAQMS can be viewed in TNPCB website\n",
      "https://tnpcb.gov.in/aqi_caaqms.php\n",
      "\n",
      "#pollution  #tnpcb #ecofriendly  #environment #sustainability #plasticfree #PlasticfreeTN\n",
      "PMK President Dr Anbumani Ramadoss criticized Tamil Nadu CM MK Stalin and demanded answers on alleged corruption links and meetings with Gautam Adani.\n",
      "\n",
      "#AdaniStalinSecretMeet\n",
      "The Ashmolean Museum at Oxford University will return a 500-year-old bronze idol of Thirumangai Alvar, the last of the 12 Alvar saints, to India. The Tamil Nadu Police successfully traced the idol's origins to India, leading to its repatriation.\n",
      "#JUSTIN \n",
      "‡Æï‡Æü‡Æ≤‡ØÇ‡Æ∞‡Øç ‡ÆÆ‡Ææ‡Æµ‡Æü‡Øç‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æ®‡Ææ‡Æ≥‡Øà (‡Æ®‡Æµ.30) \n",
      "‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø, ‡Æï‡Æ≤‡Øç‡Æ≤‡ØÇ‡Æ∞‡Æø‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ  ‡Æµ‡Æø‡Æü‡ØÅ‡ÆÆ‡ØÅ‡Æ±‡Øà\n",
      "#Cuddlore, #SchoolLeave #News18Tamilnadu | http://News18Tamil.com\n",
      "Delusional? Coming from a kong who attacks others for speaking their own language? Tamil Nadu has a long history of mistreating outsiders for not knowing Tamil. Before throwing around words like ‚Äúracist‚Äù  ‚Äúuneducated,‚Äù take a good look at your state‚Äôs track record. Hypocrisy\n",
      "Ithallam ethuku sollunga, naan sivanaee thaana irukkuran !!!\n",
      "Hey \n",
      "@FinMinIndia\n",
      " \n",
      "\n",
      "We have a bank exam (IBPS PO mains) scheduled for tomorrow \n",
      "\n",
      "But #CycloneFengal is expected to hit the coast of Tamil Nadu tomorrow (November 30) \n",
      "\n",
      "Will the exam get postponed to a later date? \n",
      "@chennaicorp\n",
      " #IBPS #ChennaiRains #IBPSPO #ChennaiRain\n",
      "The Deep Depression over Southwest Bay of Bengal intensified into Cyclonic Storm FENGAL (pronounced as FEINJAL) over the same region at 1430 hrs IST of today, 29th November.\n",
      "ADANI - STALIN SECRET MEET\n",
      "\n",
      "\"#Adani has paid bribery to Odisha, Tamil Nadu, Chattisgarh, Andhra Pradesh & J&K to get the deal passed.  \n",
      "\n",
      "The culprits who need to be jailed are Bupesh Baghel, Naveen Patnaik, Jagan Reddy, MK Stalin & LG Office in J&K\"\n",
      "\n",
      "#AdaniStalinSecretMeet\n",
      "The Cyclone Fengal will bring heavy rain, flooding, and strong winds to northern Tamil Nadu and Puducherry in India on Saturday.\n",
      "\n",
      "#CycloneFengal #ChennaiRains\n",
      "Tamil Nadu can start building the airport. Only operations should be postponed till 2033. \n",
      "\n",
      "Airports anyway need a few years for land acquisition and construction. Say 5 years. Only 4 more years of waiting.\n",
      "‚ÄúUntil we meet again dad‚Äù... ‡Æ®‡Æü‡Æø‡Æï‡Øà ‡Æö‡ÆÆ‡Æ®‡Øç‡Æ§‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æ§‡Æ®‡Øç‡Æ§‡Øà ‡Æâ‡ÆØ‡Æø‡Æ∞‡Æø‡Æ¥‡Æ®‡Øç‡Æ§‡Ææ‡Æ∞‡Øç\n",
      "\n",
      "#SamanthaRuthPrabhu #SamanthaFather #JosephPrabhu #PassedAway #ETVBharatTamil\n",
      "Tamil Nadu‚Äôs political sensation will be back on Sunday ‚Äì He is none other than Annamalai!\n",
      "\n",
      "After years of humiliation a party faced in TN, one man rose through his hard work and an unconventional and a domineering personality needed for politics!\n",
      "\n",
      "The man took his party from\n",
      "@daniel86cricket\n",
      " Kuch bolo Sir! Ya phir Sri Lanka se Tamil Nadu relocate karne ki soch rahe ho \n",
      "SOUTH AFRICA RATTLED SRI LANKA FOR 42. \n",
      "‡Æö‡ØÜ‡Æ©‡Øç‡Æ©‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ®‡Æø‡Æ≤ ‡ÆÖ‡Æ≥‡Æµ‡Æø‡Æ≤‡Ææ‡Æ© ‡ÆÖ‡Æû‡Øç‡Æö‡Æ≤‡Øç‡Æ§‡Æ≤‡Øà ‡Æï‡Æ£‡Øç‡Æï‡Ææ‡Æü‡Øç‡Æö‡Æø !\n",
      "\n",
      " ‡Æ§‡Æ®‡Ææ‡Æ™‡ØÜ‡Æï‡Øç‡Æ∏‡Øç 2025‚Äô ‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡ÆÜ‡Æ£‡Øç‡Æü‡ØÅ  ‡Æú‡Æ©‡Æµ‡Æ∞‡Æø ‡ÆÆ‡Ææ‡Æ§‡ÆÆ‡Øç 29-‡ÆÆ‡Øç ‡Æ§‡Øá‡Æ§‡Æø‡ÆØ‡Æø‡Æ≤‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æ™‡Æø‡Æ™‡Øç‡Æ∞‡Æµ‡Æ∞‡Æø ‡ÆÆ‡Ææ‡Æ§‡ÆÆ‡Øç 1-‡ÆÆ‡Øç ‡Æ§‡Øá‡Æ§‡Æø ‡Æµ‡Æ∞‡Øà ‡Æï‡Ææ‡Æ≤‡Øà 10.00 ‡ÆÆ‡Æ£‡Æø ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æá‡Æ∞‡Æµ‡ØÅ 7 ‡ÆÆ‡Æ£‡Æø ‡Æµ‡Æ∞‡Øà ‡Æö‡ØÜ‡Æ©‡Øç‡Æ©‡Øà‡ÆØ‡Æø‡Æ≤‡ØÅ‡Æ≥‡Øç‡Æ≥ ‡Æ∑‡ØÜ‡Æ©‡Ææ‡ÆØ‡Øç ‡Æ®‡Æï‡Æ∞‡Øç ‡ÆÖ‡ÆÆ‡Øç‡ÆÆ‡Ææ ‡ÆÖ‡Æ∞‡Æô‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç, ‡Æ®‡Æü‡Øà‡Æ™‡ØÜ‡Æ±‡ØÅ‡ÆÆ‡Øç.\n",
      " https://pib.gov.in/PressReleasePage.aspx?PRID=2079058‚Ä¶\n",
      "Biggest Star of Tamil Nadu\n",
      "Very important to improve one's vocabulary\n",
      "Post mangolian foods too\n",
      "Some growth about Tamilnadu, these are already posted by you earlier.\n",
      "\n",
      "Economic growth rate of Tamil Nadu's Gross State Domestic Product (GSDP) at constant prices was 8.13% in 2022-23 and 8.23% in 2023-24. This is a notable improvement from the average growth rate of 5.80% from\n",
      "Any way Tamil Nadu loss due to poor batting performance\n",
      "4.87 Lac Companies / Industries from South India (Tamil Nadu, Andhra Pradesh, Telangana, Karnataka, Kerala, Pondicherry) Data\n",
      "Call/Whatsapp For Free Sample: 8882956467\n",
      "For More Details: https://bit.ly/south-india-companies-data‚Ä¶\n",
      "#data #b2bdatabase #southindiacompaniesdata #databaseprovider #77data\n",
      "South Indian foods are very special\n",
      "Tamil Nadu is a State\n",
      "PMK leader Dr. Anbumani demands Tamil Nadu CM MK Stalin respond to allegations of shady links with Adani. \n",
      "\n",
      "#AdaniStalinSecretMeet\n",
      "PMK leader Dr. Anbumani demands Tamil Nadu CM MK Stalin respond to allegations of shady links with Adani. \n",
      "\n",
      "#AdaniStalinSecretMeet\n",
      "‡Æ®‡Æü‡Æø‡Æï‡Æ∞‡Øç ‡Æµ‡Æø‡Æú‡ÆØ‡Øç ‡ÆÆ‡Æï‡Æ©‡Øç ‡Æá‡ÆØ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æ®‡Ææ‡ÆØ‡Æï‡Æ©‡Ææ‡Æï ‡Æ®‡Æü‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æö‡Æ®‡Øç‡Æ§‡ØÄ‡Æ™‡Øç ‡Æï‡Æø‡Æ∑‡Æ©‡Øç\n",
      "\n",
      "#JasonSanjay #SundeepKishan #LycaProduction #JasonSanjay01 #ETVBharatTamil\n",
      "Latest tweets data exported to latest_tweets_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "chrome_driver_path = \"D:/chromedriver-win64/chromedriver.exe\"  # Replace with your ChromeDriver path\n",
    "service = Service(chrome_driver_path)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def search_tweets_by_geolocation(latitude, longitude, radius, place_name, min_tweets=50):\n",
    "    # Twitter search by geolocation for latest tweets\n",
    "    search_url_geo = f\"https://twitter.com/search?q=geocode%3A{latitude}%2C{longitude}%2C{radius}&f=live\"\n",
    "    \n",
    "    # Twitter search mentioning the place for latest tweets\n",
    "    search_url_place = f\"https://twitter.com/search?q={place_name}&f=live\"\n",
    "\n",
    "    # Data storage for tweets\n",
    "    tweets_data = []\n",
    "\n",
    "    # Function to collect tweets from the current page\n",
    "    def collect_tweets():\n",
    "        new_tweets = driver.find_elements(By.CSS_SELECTOR, \"[data-testid='tweetText']\")\n",
    "        for tweet in new_tweets:\n",
    "            tweet_text = tweet.text\n",
    "            tweets_data.append({\n",
    "                \"Tweet\": tweet_text,\n",
    "                \"Geolocation\": f\"{latitude}, {longitude}\"\n",
    "            })\n",
    "            print(tweet_text)\n",
    "\n",
    "    # Search for the latest tweets posted from the geolocation\n",
    "    print(f\"Searching for the most recent tweets posted from geolocation: {latitude}, {longitude} within {radius}\")\n",
    "    driver.get(search_url_geo)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Scroll and collect until at least min_tweets are gathered\n",
    "    while len(tweets_data) < min_tweets:\n",
    "        collect_tweets()\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Now search for the latest tweets mentioning the place\n",
    "    print(f\"\\nSearching for the most recent tweets mentioning: {place_name}\")\n",
    "    driver.get(search_url_place)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Scroll and collect until at least min_tweets more tweets are gathered\n",
    "    initial_count = len(tweets_data)\n",
    "    while len(tweets_data) < initial_count + min_tweets:\n",
    "        collect_tweets()\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Export the tweets to a CSV file\n",
    "    df = pd.DataFrame(tweets_data)\n",
    "    csv_filename = f\"latest_tweets_data.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Latest tweets data exported to {csv_filename}\")\n",
    "\n",
    "# Example usage with latitude, longitude, radius, and place_name\n",
    "latitude = 24.7941004\n",
    "longitude = 93.1170986\n",
    "radius = \"1000km\"  # Search within a 50 km radius\n",
    "place_name = \"Tamil Nadu\"  # Place name to search for mentions\n",
    "\n",
    "# Search and export the results\n",
    "search_tweets_by_geolocation(latitude, longitude, radius, place_name, min_tweets=50)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned tweets data exported to cleaned_tweets_data_with_punctuation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources if necessary\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the CSV file\n",
    "csv_filename = \"latest_tweets_data.csv\"\n",
    "tweets_data = pd.read_csv(csv_filename)\n",
    "\n",
    "# Ensure all entries in the Tweet column are strings (replace NaN with an empty string)\n",
    "tweets_data['Tweet'] = tweets_data['Tweet'].fillna('').astype(str)\n",
    "\n",
    "# Function to clean tweet text while keeping punctuations like !, ?, .\n",
    "def clean_tweet(text):\n",
    "    try:\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove mentions (@username) and hashtags (#hashtag)\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        \n",
    "        # Remove non-ASCII characters (non-English characters)\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        \n",
    "        # Remove special characters except for meaningful punctuation (!, ?, ., etc.)\n",
    "        text = re.sub(r\"[^\\w\\s!?.,]\", '', text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweet: {text}, Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "# Optional: Remove stopwords (if desired)\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the cleaning function to the tweets\n",
    "tweets_data['Cleaned_Tweet'] = tweets_data['Tweet'].apply(clean_tweet)\n",
    "\n",
    "# Optional: Uncomment the next line to remove stopwords\n",
    "# tweets_data['Cleaned_Tweet'] = tweets_data['Cleaned_Tweet'].apply(remove_stopwords)\n",
    "\n",
    "# Save the cleaned tweets back to a new CSV file\n",
    "cleaned_csv_filename = \"cleaned_tweets_data_with_punctuation.csv\"\n",
    "tweets_data.to_csv(cleaned_csv_filename, index=False)\n",
    "\n",
    "print(f\"Cleaned tweets data exported to {cleaned_csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Cleaned_Tweet\n",
      "0  had a very good meeting with legislators and m...\n",
      "1                 hope you had a great thanksgiving!\n",
      "2  elon musk, donald trump, barron and melania bl...\n",
      "3  i bow to the land of mahaprabhu jagannath. the...\n",
      "4  37yearold lionel messi is still being nominate...\n",
      "5            you are so much more than the media now\n",
      "6  the indian cricket team were hosted by the hon...\n",
      "7  jude bellingham waited by liverpools dressing ...\n",
      "8                                               dave\n",
      "9                                        back tattoo\n"
     ]
    }
   ],
   "source": [
    "# Display the first few entries of the cleaned data\n",
    "try:\n",
    "    cleaned_csv_filename = \"cleaned_tweets_data_with_punctuation.csv\"\n",
    "    cleaned_tweets_data = pd.read_csv(cleaned_csv_filename)\n",
    "    cleaned_tweets_data.head()\n",
    "    print(tweets_data[['Cleaned_Tweet']].head(10))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{cleaned_csv_filename}' not found. Ensure the cleaning process ran successfully and the file exists.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets after filtering: 87\n",
      "Map saved as cleaned_tweets_map.html. Open it in a browser to view.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.element import Figure\n",
    "\n",
    "# Load the cleaned tweets data\n",
    "cleaned_csv_filename = \"cleaned_tweets_data_with_punctuation.csv\"\n",
    "tweets_data = pd.read_csv(cleaned_csv_filename)\n",
    "\n",
    "# Ensure all entries in the Tweet column are strings (replace NaN with an empty string)\n",
    "tweets_data['Cleaned_Tweet'] = tweets_data['Cleaned_Tweet'].fillna('').astype(str)\n",
    "\n",
    "# Generate a popularity metric (e.g., length of the cleaned tweet text)\n",
    "tweets_data['Popularity'] = tweets_data['Cleaned_Tweet'].apply(len)\n",
    "\n",
    "# Drop tweets with cleaned length shorter than 10 characters\n",
    "tweets_data = tweets_data[tweets_data['Popularity'] >= 10]\n",
    "\n",
    "# Parse geolocation into separate Latitude and Longitude columns\n",
    "tweets_data[['Latitude', 'Longitude']] = tweets_data['Geolocation'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "# Ensure all filtered entries are being used\n",
    "print(f\"Total tweets after filtering: {len(tweets_data)}\")  # Logs the count of tweets being processed\n",
    "\n",
    "# Calculate the center of the map based on all tweet locations\n",
    "map_center = [tweets_data['Latitude'].mean(), tweets_data['Longitude'].mean()]\n",
    "\n",
    "# Initialize the folium map\n",
    "fig = Figure(width=800, height=600)\n",
    "tweet_map = folium.Map(location=map_center, zoom_start=6)\n",
    "fig.add_child(tweet_map)\n",
    "\n",
    "# Add a marker cluster to the map for the general tweet area\n",
    "marker_cluster = MarkerCluster().add_to(tweet_map)\n",
    "\n",
    "# Tweet-styled popup HTML template\n",
    "def create_tweet_popup(tweet_text):\n",
    "    return folium.Popup(\n",
    "        html=f\"\"\"\n",
    "        <div style=\"font-family: Arial, sans-serif; font-size: 12px; line-height: 1.5; max-width: 300px; padding: 10px; \n",
    "                    border: 1px solid #e1e8ed; border-radius: 10px; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\">\n",
    "            <div style=\"display: flex; align-items: center; margin-bottom: 10px;\">\n",
    "                <img src=\"https://cdn-icons-png.flaticon.com/512/733/733579.png\" width=\"30\" height=\"30\" style=\"margin-right: 10px; border-radius: 50%;\">\n",
    "                <b style=\"color: #1da1f2;\">TwitterTracker</b>\n",
    "            </div>\n",
    "            <p style=\"margin: 0; color: #14171a;\">{tweet_text}</p>\n",
    "            <hr style=\"border: none; border-top: 1px solid #e1e8ed; margin: 10px 0;\">\n",
    "            <div style=\"display: flex; justify-content: space-between; font-size: 10px; color: #657786;\">\n",
    "                <span>12:34 PM ¬∑ Nov 29, 2024</span>\n",
    "                <span>Twitter Web App</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\",\n",
    "        max_width=350,\n",
    "    )\n",
    "\n",
    "# Add all tweet locations to the map using styled popups\n",
    "for _, tweet in tweets_data.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[tweet['Latitude'], tweet['Longitude']],\n",
    "        popup=create_tweet_popup(tweet['Cleaned_Tweet']),\n",
    "        icon=folium.Icon(color=\"blue\", icon=\"info-sign\"),\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Highlight the top tweets with tweet-styled popups and custom icons\n",
    "top_tweets = tweets_data.nlargest(10, 'Popularity')\n",
    "for _, tweet in top_tweets.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[tweet['Latitude'], tweet['Longitude']],\n",
    "        popup=create_tweet_popup(tweet['Cleaned_Tweet']),\n",
    "        icon=folium.CustomIcon(\n",
    "            \"https://cdn-icons-png.flaticon.com/512/733/733579.png\",  # Twitter logo\n",
    "            icon_size=(30, 30)\n",
    "        ),\n",
    "    ).add_to(tweet_map)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "tweet_map.save(\"cleaned_tweets_map.html\")\n",
    "\n",
    "print(\"Map saved as cleaned_tweets_map.html. Open it in a browser to view.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mehul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1915 features, but MLPClassifier is expecting 5000 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_mlp_model.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Make sure to replace with the actual path\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Predict whether each tweet is a crisis or not\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Add predictions to the dataframe\u001b[39;00m\n\u001b[0;32m     79\u001b[0m tweets_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_pred\n",
      "File \u001b[1;32mc:\\Users\\mehul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1163\u001b[0m, in \u001b[0;36mMLPClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the multi-layer perceptron classifier.\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m \n\u001b[0;32m   1152\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;124;03m    The predicted classes.\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1167\u001b[0m, in \u001b[0;36mMLPClassifier._predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Private predict method with optional input validation\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1167\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_pass_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1170\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mc:\\Users\\mehul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:206\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the trained model\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mThis is the same as _forward_pass but does not record the activations\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    The decision function of the samples for each class in the model.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[1;32m--> 206\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Initialize first layer\u001b[39;00m\n\u001b[0;32m    209\u001b[0m activation \u001b[38;5;241m=\u001b[39m X\n",
      "File \u001b[1;32mc:\\Users\\mehul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\mehul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1915 features, but MLPClassifier is expecting 5000 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Download stopwords and lemmatizer data if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Compile regular expressions once for optimization\n",
    "mention_pattern = re.compile(r'@\\w+')\n",
    "hashtag_pattern = re.compile(r'#')\n",
    "url_pattern = re.compile(r\"http\\S+|www\\S+|https\\S+\")\n",
    "emoji_pattern = re.compile(r'[\\U00010000-\\U0010FFFF]')\n",
    "special_char_pattern = re.compile(r'[^A-Za-z\\s]')\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_tweet_optimized(tweet):\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Remove mentions\n",
    "    tweet = mention_pattern.sub('', tweet)\n",
    "    # Remove \"#\" symbol but retain the text\n",
    "    tweet = hashtag_pattern.sub('', tweet)\n",
    "    # Remove URLs\n",
    "    tweet = url_pattern.sub('', tweet)\n",
    "    # Remove emojis\n",
    "    tweet = emoji_pattern.sub('', tweet)\n",
    "    # Remove special characters, numbers, and punctuation (except words)\n",
    "    tweet = special_char_pattern.sub('', tweet)\n",
    "    # Tokenize and remove stopwords\n",
    "    words = [word for word in tweet.split() if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load the cleaned tweets data\n",
    "cleaned_csv_filename = \"cleaned_tweets_data_with_punctuation.csv\"\n",
    "tweets_data = pd.read_csv(cleaned_csv_filename)\n",
    "\n",
    "# Ensure all entries in the Tweet column are strings (replace NaN with an empty string)\n",
    "tweets_data['Cleaned_Tweet'] = tweets_data['Cleaned_Tweet'].fillna('').astype(str)\n",
    "\n",
    "# Generate a popularity metric (e.g., length of the cleaned tweet text)\n",
    "tweets_data['Popularity'] = tweets_data['Cleaned_Tweet'].apply(len)\n",
    "\n",
    "# Drop tweets with cleaned length shorter than 10 characters\n",
    "tweets_data = tweets_data[tweets_data['Popularity'] >= 10]\n",
    "\n",
    "# Parse geolocation into separate Latitude and Longitude columns\n",
    "tweets_data[['Latitude', 'Longitude']] = tweets_data['Geolocation'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "# Apply the preprocessing function to each tweet in the 'Cleaned_Tweet' column\n",
    "tweets_data['cleaned_tweet'] = tweets_data['Cleaned_Tweet'].apply(preprocess_tweet_optimized)\n",
    "\n",
    "\n",
    "# Load the saved TF-IDF Vectorizer\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Vectorize the cleaned tweets\n",
    "X = tfidf_vectorizer.fit_transform(tweets_data['cleaned_tweet'])\n",
    "\n",
    "# Load your pretrained model (this assumes you've saved it in a file)\n",
    "pretrained_model = joblib.load(\"final_mlp_model.joblib\")  # Make sure to replace with the actual path\n",
    "\n",
    "# Predict whether each tweet is a crisis or not\n",
    "y_pred = pretrained_model.predict(X)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "tweets_data['Prediction'] = y_pred\n",
    "\n",
    "# Filter out non-crisis tweets\n",
    "crisis_tweets = tweets_data[tweets_data['Prediction'] == 1]  # Assuming '1' indicates a crisis tweet\n",
    "\n",
    "# Print the filtered crisis tweets\n",
    "print(crisis_tweets[['Cleaned_Tweet', 'Prediction']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load a sample dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train a simple Logistic Regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model using joblib\n",
    "joblib.dump(model, 'sample_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
